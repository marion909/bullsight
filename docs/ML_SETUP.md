# BullSight ML Dart Detection Setup

## Ãœberblick

BullSight kann ML-basierte Dart-Erkennung mit YOLOv8 verwenden. Diese ist wesentlich robuster als klassische Computer Vision bei:
- Unterschiedlichen LichtverhÃ¤ltnissen
- Verschiedenen Dart-Typen und -Farben
- SchrÃ¤gen Kamerawinkeln
- Teilverdeckungen

## ğŸ¯ ML Demo Modus

BullSight hat einen integrierten **ML Demo Modus** zum Testen und Visualisieren der Dart-Erkennung!

### Demo Modus starten:

1. Ã–ffne BullSight
2. Klicke auf **"ğŸ¤– ML Detection Demo"** im HauptmenÃ¼
3. WÃ¤hle zwischen:
   - **Live Camera**: Echtzeit-Erkennung vom Kamera-Feed
   - **Test Image**: Lade gespeicherte Bilder zum Testen

### Features des Demo Modus:

- âœ… **Live-Visualisierung**: Sieh die ML-Erkennung in Echtzeit
- âœ… **Bounding Boxes**: GrÃ¼ne/gelbe/orange Boxen je nach Confidence
- âœ… **Confidence Scores**: Prozent-Anzeige fÃ¼r jede Erkennung
- âœ… **Positionsanzeige**: Pixel-Koordinaten und Dartboard-Feld
- âœ… **Confidence Threshold**: Slider zum Anpassen (10-95%)
- âœ… **Board Overlay**: Zeige Kalibrierungsringe an
- âœ… **Multi-Dart**: Erkennt mehrere Darts gleichzeitig

### Farb-Kodierung:

- ğŸŸ¢ **GrÃ¼n**: Hohe Confidence (>70%)
- ğŸŸ¡ **Gelb**: Mittlere Confidence (50-70%)
- ğŸŸ  **Orange**: Niedrige Confidence (<50%)

## Installation

### 1. ML-AbhÃ¤ngigkeiten installieren

```bash
pip install -r requirements-ml.txt
```

Dies installiert:
- `ultralytics` (YOLOv8)
- `torch` (PyTorch)
- `torchvision`

### 2. Model Options

#### Option A: Schnellstart mit vortrainiertem Modell (Empfohlen)

Das YOLOv8-nano Basismodell wird automatisch heruntergeladen. Es funktioniert bereits fÃ¼r Objekt-Erkennung, braucht aber Fine-Tuning fÃ¼r optimale Dart-Detection.

**Aktivieren:**
```python
# In src/vision/dart_detector.py __init__:
detector = DartDetector(use_ml=True)
```

#### Option B: Eigenes Modell trainieren (Beste Genauigkeit)

1. **Bilder sammeln** (mindestens 50-100):
   - Verwende die "Capture Test Image" Funktion im Kalibrierungsscreen
   - Verschiedene Dart-Positionen auf der Scheibe
   - Verschiedene LichtverhÃ¤ltnisse
   - Speicherort: `test_images/`

2. **Bilder annotieren**:
   - Verwende [LabelImg](https://github.com/heartexlabs/labelImg) oder [Roboflow](https://roboflow.com)
   - Markiere die Dart-Spitze mit einer Bounding Box
   - Label: "dart"
   - Export im YOLO-Format

3. **Dataset strukturieren**:
   ```
   dataset/
   â”œâ”€â”€ images/
   â”‚   â”œâ”€â”€ train/
   â”‚   â””â”€â”€ val/
   â”œâ”€â”€ labels/
   â”‚   â”œâ”€â”€ train/
   â”‚   â””â”€â”€ val/
   â””â”€â”€ data.yaml
   ```

4. **data.yaml erstellen**:
   ```yaml
   path: /pfad/zu/dataset
   train: images/train
   val: images/val
   
   nc: 1  # Number of classes
   names: ['dart']
   ```

5. **Modell trainieren**:
   ```python
   from src.vision.ml_dart_detector import train_model
   
   train_model(
       data_yaml='dataset/data.yaml',
       epochs=100,
       model_size='n'  # n=nano, s=small, m=medium
   )
   ```

6. **Trainiertes Modell verwenden**:
   ```python
   detector = DartDetector(
       use_ml=True,
       ml_model_path='bullsight_training/dart_detector/weights/best.pt'
   )
   ```

## Aktivierung in BullSight

### Automatisch (Empfohlen)

ML wird **automatisch aktiviert** wenn Ultralytics installiert ist! Einfach:

```bash
pip install ultralytics torch torchvision
python -m src.main
```

Das war's! BullSight erkennt automatisch dass ML verfÃ¼gbar ist.

### Manuell (Optional)

Falls du ML manuell steuern mÃ¶chtest:

```bash
# ML explizit aktivieren
$env:BULLSIGHT_USE_ML=1
python -m src.main

# ML explizit deaktivieren (auch wenn installiert)
$env:BULLSIGHT_USE_ML=0
python -m src.main

# Eigenes Modell verwenden
$env:BULLSIGHT_ML_MODEL="pfad/zu/model.pt"
$env:BULLSIGHT_ML_CONFIDENCE=0.6
python -m src.main
```

### Via Code

In `src/main.py`:

```python
# ML Detection aktivieren
self.detector = DartDetector(
    use_ml=True,  # ML aktivieren
    ml_model_path='models/dart_detector.pt',  # Optional: eigenes Modell
    ml_confidence=0.5  # Mindest-Konfidenz (0.0-1.0)
)
```

## Performance

### Raspberry Pi 4

- **YOLOv8-nano**: ~100ms pro Frame (gut nutzbar)
- **YOLOv8-small**: ~250ms pro Frame (langsamer aber genauer)

### Desktop/Laptop

- **YOLOv8-nano**: ~20-50ms pro Frame
- **YOLOv8-small**: ~50-100ms pro Frame

## Troubleshooting

### ImportError: No module named 'ultralytics'

```bash
pip install ultralytics torch torchvision
```

### CUDA out of memory

Verwende kleineres Modell (nano statt small) oder reduziere BildgrÃ¶ÃŸe.

### Schlechte Erkennungsrate

1. Sammle mehr Trainingsbilder
2. Achte auf BildqualitÃ¤t (Beleuchtung, SchÃ¤rfe)
3. Annotiere prÃ¤zise (nur die Dart-Spitze)
4. Trainiere lÃ¤nger (mehr Epochs)

## Vergleich: Classical CV vs ML

| Aspekt | Classical CV | ML (YOLO) |
|--------|-------------|-----------|
| Setup  | Sofort bereit | Training nÃ¶tig |
| Robustheit | MittelmÃ¤ÃŸig | Sehr gut |
| Licht-Varianz | Empfindlich | Robust |
| Geschwindigkeit | Sehr schnell | Schnell |
| Winkel-Toleranz | Begrenzt | Sehr gut |
| Mehrfach-Darts | MÃ¼hsam | Einfach |

## Best Practices

1. **Start mit Classical CV**: Teste erst das bestehende System
2. **Sammle Daten wÃ¤hrend Nutzung**: Nutze "Capture Test Image"
3. **Iteratives Training**: Training â†’ Test â†’ Mehr Daten â†’ Retraining
4. **Data Augmentation**: Roboflow bietet automatische Augmentation
5. **Hybrid-Ansatz**: ML als Primary, Classical CV als Fallback

## NÃ¤chste Schritte

1. â˜ ML-AbhÃ¤ngigkeiten installieren
2. â˜ 50-100 Dart-Bilder sammeln
3. â˜ Bilder mit LabelImg/Roboflow annotieren
4. â˜ Dataset vorbereiten (train/val split)
5. â˜ Modell trainieren (1-2 Stunden)
6. â˜ In BullSight aktivieren und testen
7. â˜ Bei Bedarf: mehr Daten sammeln und nachtrainieren
